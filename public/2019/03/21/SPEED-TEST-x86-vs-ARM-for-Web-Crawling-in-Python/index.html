<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">









  <meta name="keywords" content="Python,LCTT 翻译,x86,ARM,爬虫,">










<meta name="description" content="假如说，如果你的老板给你的任务是一次又一次地访问竞争对手的网站，把对方商品的价格记录下来，而且要纯手工操作，恐怕你会想要把整个办公室都烧掉。 之所以现在网络爬虫的影响力如此巨大，就是因为网络爬虫可以被用于追踪客户的情绪和趋向、搜寻空缺的职位、监控房地产的交易，甚至是获取 UFC 的比赛结果。除此以外，还有很多意想不到的用途。 对于有这方面爱好的人来说，爬虫无疑是一个很好的工具。因此，我使用了 S">
<meta name="keywords" content="Python,LCTT 翻译,x86,ARM,爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="x86 和 ARM 的 Python 爬虫速度对比">
<meta property="og:url" content="http://github.hankchow.net/2019/03/21/SPEED-TEST-x86-vs-ARM-for-Web-Crawling-in-Python/index.html">
<meta property="og:site_name" content="HankChow&#39;s Blog">
<meta property="og:description" content="假如说，如果你的老板给你的任务是一次又一次地访问竞争对手的网站，把对方商品的价格记录下来，而且要纯手工操作，恐怕你会想要把整个办公室都烧掉。 之所以现在网络爬虫的影响力如此巨大，就是因为网络爬虫可以被用于追踪客户的情绪和趋向、搜寻空缺的职位、监控房地产的交易，甚至是获取 UFC 的比赛结果。除此以外，还有很多意想不到的用途。 对于有这方面爱好的人来说，爬虫无疑是一个很好的工具。因此，我使用了 S">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://blog.dxmtechsupport.com.au/wp-content/uploads/2019/02/quadbike-1024x683.jpg">
<meta property="og:image" content="https://blog.dxmtechsupport.com.au/wp-content/uploads/2019/02/Screenshot-from-2019-02-16-17-01-08.png">
<meta property="og:updated_time" content="2019-10-12T08:11:41.446Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="x86 和 ARM 的 Python 爬虫速度对比">
<meta name="twitter:description" content="假如说，如果你的老板给你的任务是一次又一次地访问竞争对手的网站，把对方商品的价格记录下来，而且要纯手工操作，恐怕你会想要把整个办公室都烧掉。 之所以现在网络爬虫的影响力如此巨大，就是因为网络爬虫可以被用于追踪客户的情绪和趋向、搜寻空缺的职位、监控房地产的交易，甚至是获取 UFC 的比赛结果。除此以外，还有很多意想不到的用途。 对于有这方面爱好的人来说，爬虫无疑是一个很好的工具。因此，我使用了 S">
<meta name="twitter:image" content="https://blog.dxmtechsupport.com.au/wp-content/uploads/2019/02/quadbike-1024x683.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://github.hankchow.net/2019/03/21/SPEED-TEST-x86-vs-ARM-for-Web-Crawling-in-Python/">





  <title>x86 和 ARM 的 Python 爬虫速度对比 | HankChow's Blog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">HankChow's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://github.hankchow.net/2019/03/21/SPEED-TEST-x86-vs-ARM-for-Web-Crawling-in-Python/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="HankChow">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HankChow's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">x86 和 ARM 的 Python 爬虫速度对比</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T21:29:18+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  23
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://blog.dxmtechsupport.com.au/wp-content/uploads/2019/02/quadbike-1024x683.jpg" alt=""></p>
<p>假如说，如果你的老板给你的任务是一次又一次地访问竞争对手的网站，把对方商品的价格记录下来，而且要纯手工操作，恐怕你会想要把整个办公室都烧掉。</p>
<p>之所以现在网络爬虫的影响力如此巨大，就是因为网络爬虫可以被用于追踪客户的情绪和趋向、搜寻空缺的职位、监控房地产的交易，甚至是获取 UFC 的比赛结果。除此以外，还有很多意想不到的用途。</p>
<p>对于有这方面爱好的人来说，爬虫无疑是一个很好的工具。因此，我使用了 <a href="https://scrapy.org/" target="_blank" rel="noopener">Scrapy</a> 这个基于 Python 编写的开源网络爬虫框架。</p>
<p>鉴于我不太了解这个工具是否会对我的计算机造成伤害，我并没有将它搭建在我的主力机器上，而是搭建在了一台树莓派上面。</p>
<p>令人感到意外的是，Scrapy 在树莓派上面的性能并不差，或许这是 ARM 架构服务器的又一个成功例子？</p>
<p>我尝试 Google 了一下，但并没有得到令我满意的结果，仅仅找到了一篇相关的《<a href="https://www.info2007.net/blog/2018/review-scaleway-arm-based-cloud-server.html" target="_blank" rel="noopener">Drupal 建站对比</a>》。这篇文章的结论是，ARM 架构服务器性能比昂贵的 x86 架构服务器要更好。</p>
<p>从另一个角度来看，这种 web 服务可以看作是一个“被爬虫”服务，但和 Scrapy 对比起来，前者是基于 LAMP 技术栈，而后者则依赖于 Python，这就导致两者之间没有太多的可比性。</p>
<p>那我们该怎样做呢？只能在一些 VPS 上搭建服务来对比一下了。</p>
<h3 id="什么是-ARM-架构处理器？"><a href="#什么是-ARM-架构处理器？" class="headerlink" title="什么是 ARM 架构处理器？"></a>什么是 ARM 架构处理器？</h3><p>ARM 是目前世界上最流行的 CPU 架构。</p>
<p>但 ARM 架构处理器在很多人眼中的地位只是作为一个省钱又省电的选择，而不是跑在生产环境中的处理器的首选。</p>
<p>然而，诞生于英国剑桥的 ARM CPU，最初是用于极其昂贵的 <a href="https://blog.dxmtechsupport.com.au/playing-badass-acorn-archimedes-games-on-a-raspberry-pi/" target="_blank" rel="noopener">Acorn Archimedes</a> 计算机上的，这是当时世界上最强大的桌面计算机，甚至在很长一段时间内，它的运算速度甚至比最快的 386 还要快好几倍。</p>
<p>Acorn 公司和 Commodore、Atari 的理念类似，他们认为一家伟大的计算机公司就应该制造出伟大的计算机，让人感觉有点目光短浅。而比尔盖茨的想法则有所不同，他力图在更多不同种类和价格的 x86 机器上使用他的 DOS 系统。</p>
<p>拥有大量用户基数的平台会成为第三方开发者开发软件的平台，而软件资源丰富又会让你的计算机更受用户欢迎。</p>
<p>即使是苹果公司也几乎被打败。在 x86 芯片上投入大量的财力，最终，这些芯片被用于生产环境计算任务。</p>
<p>但 ARM 架构也并没有消失。基于 ARM 架构的芯片不仅运算速度快，同时也非常节能。因此诸如机顶盒、PDA、数码相机、MP3 播放器这些电子产品多数都会采用 ARM 架构的芯片，甚至在很多需要用电池或不配备大散热风扇的电子产品上，都可以见到 ARM 芯片的身影。</p>
<p>而 ARM 则脱离 Acorn 成为了一种特殊的商业模式，他们不生产实物芯片，仅仅是向芯片生产厂商出售相关的知识产权。</p>
<p>因此，这或多或少是 ARM 芯片被应用于如此之多的手机和平板电脑上的原因。当 Linux 被移植到这种架构的芯片上时，开源技术的大门就已经向它打开了，这才让我们今天得以在这些芯片上运行 web 爬虫程序。</p>
<h4 id="服务器端的-ARM"><a href="#服务器端的-ARM" class="headerlink" title="服务器端的 ARM"></a>服务器端的 ARM</h4><p>诸如<a href="https://www.computerworld.com/article/3178544/microsoft-windows/microsoft-and-arm-look-to-topple-intel-in-servers.html" target="_blank" rel="noopener">微软</a>和 <a href="https://www.datacenterknowledge.com/design/cloudflare-bets-arm-servers-it-expands-its-data-center-network" target="_blank" rel="noopener">Cloudflare</a> 这些大厂都在基础设施建设上花了重金，所以对于我们这些预算不高的用户来说，可以选择的余地并不多。</p>
<p>实际上，如果你的信用卡只够付每月数美元的 VPS 费用，一直以来只能考虑 <a href="https://www.scaleway.com/" target="_blank" rel="noopener">Scaleway</a> 这个高性价比的厂商。</p>
<p>但自从数个月前公有云巨头 <a href="https://aws.amazon.com/" target="_blank" rel="noopener">AWS</a> 推出了他们自研的 ARM 处理器 <a href="https://www.theregister.co.uk/2018/11/27/amazon_aws_graviton_specs/" target="_blank" rel="noopener">AWS Graviton</a> 之后，选择似乎就丰富了一些。</p>
<p>我决定在其中选择一款 VPS 厂商，将它提供的 ARM 处理器和 x86 处理器作出对比。</p>
<h3 id="深入了解"><a href="#深入了解" class="headerlink" title="深入了解"></a>深入了解</h3><p>所以我们要对比的是什么指标呢？</p>
<h4 id="Scaleway"><a href="#Scaleway" class="headerlink" title="Scaleway"></a>Scaleway</h4><p>Scaleway 自身的定位是“专为开发者设计”。我觉得这个定位很准确，对于开发和原型设计来说，Scaleway 提供的产品确实可以作为一个很好的沙盒环境。</p>
<p>Scaleway 提供了一个简洁的仪表盘页面，让用户可以快速地从主页进入 bash shell 界面。对于很多小企业、自由职业者或者技术顾问，如果想要运行 web 爬虫，这个产品毫无疑问是一个物美价廉的选择。</p>
<p>ARM 方面我们选择 <a href="https://www.scaleway.com/virtual-cloud-servers/#anchor_arm" target="_blank" rel="noopener">ARM64-2GB</a> 这一款服务器，每月只需要 3 欧元。它带有 4 个 Cavium ThunderX 核心，这是在 2014 年推出的第一款服务器级的 ARMv8 处理器。但现在看来它已经显得有点落后了，并逐渐被更新的 ThunderX2 取代。</p>
<p>x86 方面我们选择 <a href="https://www.scaleway.com/virtual-cloud-servers/#anchor_starter" target="_blank" rel="noopener">1-S</a>，每月的费用是 4 欧元。它拥有 2 个英特尔 Atom C3995 核心。英特尔的 Atom 系列处理器的特点是低功耗、单线程，最初是用在笔记本电脑上的，后来也被服务器所采用。</p>
<p>两者在处理器以外的条件都大致相同，都使用 2 GB 的内存、50 GB 的 SSD 存储以及 200 Mbit/s 的带宽。磁盘驱动器可能会有所不同，但由于我们运行的是 web 爬虫，基本都是在内存中完成操作，因此这方面的差异可以忽略不计。</p>
<p>为了避免我不能熟练使用包管理器的尴尬局面，两方的操作系统我都会选择使用 Debian 9。</p>
<h4 id="Amazon-Web-Services（AWS）"><a href="#Amazon-Web-Services（AWS）" class="headerlink" title="Amazon Web Services（AWS）"></a>Amazon Web Services（AWS）</h4><p>当你还在注册 AWS 账号的时候，使用 Scaleway 的用户可能已经把提交信用卡信息、启动 VPS 实例、添加 sudo 用户、安装依赖包这一系列流程都完成了。AWS 的操作相对来说比较繁琐，甚至需要详细阅读手册才能知道你正在做什么。</p>
<p>当然这也是合理的，对于一些需求复杂或者特殊的企业用户，确实需要通过详细的配置来定制合适的使用方案。</p>
<p>我们所采用的 AWS Graviton 处理器是 AWS EC2（<ruby>弹性计算云<rt>Elastic Compute Cloud</rt></ruby>）的一部分，我会以按需实例的方式来运行，这也是最贵但最简捷的方式。AWS 同时也提供<a href="https://aws.amazon.com/ec2/spot/pricing/" target="_blank" rel="noopener">竞价实例</a>，这样可以用较低的价格运行实例，但实例的运行时间并不固定。如果实例需要长时间持续运行，还可以选择<a href="https://aws.amazon.com/ec2/pricing/reserved-instances/" target="_blank" rel="noopener">预留实例</a>。</p>
<p>看，AWS 就是这么复杂……</p>
<p>我们分别选择 <a href="https://aws.amazon.com/ec2/instance-types/a1/" target="_blank" rel="noopener">a1.medium</a> 和 <a href="https://aws.amazon.com/ec2/instance-types/t2/" target="_blank" rel="noopener">t2.small</a> 两种型号的实例进行对比，两者都带有 2GB 内存。这个时候问题来了，这里提到的 vCPU 又是什么？两种型号的不同之处就在于此。</p>
<p>对于 a1.medium 型号的实例，vCPU 是 AWS Graviton 芯片提供的单个计算核心。这个芯片由被亚马逊在 2015 收购的以色列厂商 Annapurna Labs 研发，是 AWS 独有的单线程 64 位 ARMv8 内核。它的按需价格为每小时 0.0255 美元。</p>
<p>而 t2.small 型号实例使用英特尔至强系列芯片，但我不确定具体是其中的哪一款。它每个核心有两个线程，但我们并不能用到整个核心，甚至整个线程。</p>
<p>我们能用到的只是“20% 的基准性能，可以使用 CPU 积分突破这个基准”。这可能有一定的原因，但我没有弄懂。它的按需价格是每小时 0.023 美元。</p>
<p>在镜像库中没有 Debian 发行版的镜像，因此我选择了 Ubuntu 18.04。</p>
<h3 id="瘪四与大头蛋爬取-Moz-排行榜前-500-的网站"><a href="#瘪四与大头蛋爬取-Moz-排行榜前-500-的网站" class="headerlink" title="瘪四与大头蛋爬取 Moz 排行榜前 500 的网站"></a>瘪四与大头蛋爬取 Moz 排行榜前 500 的网站</h3><p>要测试这些 VPS 的 CPU 性能，就该使用爬虫了。一个方法是对几个网站在尽可能短的时间里发出尽可能多的请求，但这种操作不太礼貌，我的做法是只向大量网站发出少数几个请求。</p>
<p>为此，我编写了 <code>beavis.py</code>（瘪四）这个爬虫程序（致敬我最喜欢的物理学家和制片人 Mike Judge）。这个程序会将 Moz 上排行前 500 的网站都爬取 3 层的深度，并计算 “wood” 和 “ass” 这两个单词在 HTML 文件中出现的次数。（LCTT 译注：beavis（瘪四）和 butt-head（大头蛋） 都是 Mike Judge 的动画片《瘪四与大头蛋》中的角色）</p>
<p>但我实际爬取的网站可能不足 500 个，因为我需要遵循网站的 <code>robot.txt</code> 协定，另外还有些网站需要提交 javascript 请求，也不一定会计算在内。但这已经是一个足以让 CPU 保持繁忙的爬虫任务了。</p>
<p>Python 的<a href="https://wiki.python.org/moin/GlobalInterpreterLock" target="_blank" rel="noopener">全局解释器锁</a>机制会让我的程序只能用到一个 CPU 线程。为了测试多线程的性能，我需要启动多个独立的爬虫程序进程。</p>
<p>因此我还编写了 <code>butthead.py</code>，尽管大头蛋很粗鲁，它也总是比瘪四要略胜一筹。</p>
<p>我将整个爬虫任务拆分为多个部分，这可能会对爬取到的链接数量有一点轻微的影响。但无论如何，每次爬取都会有所不同，我们要关注的是爬取了多少个页面，以及耗时多长。</p>
<h3 id="在-ARM-服务器上安装-Scrapy"><a href="#在-ARM-服务器上安装-Scrapy" class="headerlink" title="在 ARM 服务器上安装 Scrapy"></a>在 ARM 服务器上安装 Scrapy</h3><p>安装 Scrapy 的过程与芯片的不同架构没有太大的关系，都是安装 <code>pip</code> 和相关的依赖包之后，再使用 <code>pip</code> 来安装 Scrapy。</p>
<p>据我观察，在使用 ARM 的机器上使用 <code>pip</code> 安装 Scrapy 确实耗时要长一点，我估计是由于需要从源码编译为二进制文件。</p>
<p>在 Scrapy 安装结束后，就可以通过 shell 来查看它的工作状态了。</p>
<p>在 Scaleway 的 ARM 机器上，Scrapy 安装完成后会无法正常运行，这似乎和 <code>service_identity</code> 模块有关。这个现象也会在树莓派上出现，但在 AWS Graviton 上不会出现。</p>
<p>对于这个问题，可以用这个命令来解决：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install service_identity --force --upgrade</span><br></pre></td></tr></table></figure>
<p>接下来就可以开始对比了。</p>
<h3 id="单线程爬虫"><a href="#单线程爬虫" class="headerlink" title="单线程爬虫"></a>单线程爬虫</h3><p>Scrapy 的官方文档建议<a href="https://docs.scrapy.org/en/latest/topics/broad-crawls.html" target="_blank" rel="noopener">将爬虫程序的 CPU 使用率控制在 80% 到 90% 之间</a>，在真实操作中并不容易，尤其是对于我自己写的代码。根据我的观察，实际的 CPU 使用率变动情况是一开始非常繁忙，随后稍微下降，接着又再次升高。</p>
<p>在爬取任务的最后，也就是大部分目标网站都已经被爬取了的这个阶段，会持续数分钟的时间。这让人有点失望，因为在这个阶段当中，任务的运行时长只和网站的大小有比较直接的关系，并不能以之衡量 CPU 的性能。</p>
<p>所以这并不是一次严谨的基准测试，只是我通过自己写的爬虫程序来观察实际的现象。</p>
<p>下面我们来看看最终的结果。首先是 Scaleway 的机器：</p>
<table>
<thead>
<tr>
<th>机器种类</th>
<th>耗时</th>
<th>爬取页面数</th>
<th>每小时爬取页面数</th>
<th>每百万页面费用（欧元）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scaleway ARM64-2GB</td>
<td>108m 59.27s</td>
<td>38,205</td>
<td>21,032.623</td>
<td>0.28527</td>
</tr>
<tr>
<td>Scaleway 1-S</td>
<td>97m 44.067s</td>
<td>39,476</td>
<td>24,324.648</td>
<td>0.33011</td>
</tr>
</tbody>
</table>
<p>我使用了 <a href="https://linux.die.net/man/1/top" target="_blank" rel="noopener">top</a> 工具来查看爬虫程序运行期间的 CPU 使用率。在任务刚开始的时候，两者的 CPU 使用率都达到了 100%，但 ThunderX 大部分时间都达到了 CPU 的极限，无法看出来 Atom 的性能会比 ThunderX 超出多少。</p>
<p>通过 <code>top</code> 工具，我还观察了它们的内存使用情况。随着爬取任务的进行，ARM 机器的内存使用率最终达到了 14.7%，而 x86 则最终是 15%。</p>
<p>从运行日志还可以看出来，当 CPU 使用率到达极限时，会有大量的超时页面产生，最终导致页面丢失。这也是合理出现的现象，因为 CPU 过于繁忙会无法完整地记录所有爬取到的页面。</p>
<p>如果仅仅是为了对比爬虫的速度，页面丢失并不是什么大问题。但在实际中，业务成果和爬虫数据的质量是息息相关的，因此必须为 CPU 留出一些用量，以防出现这种现象。</p>
<p>再来看看 AWS 这边：</p>
<table>
<thead>
<tr>
<th>机器种类</th>
<th>耗时</th>
<th>爬取页面数</th>
<th>每小时爬取页面数</th>
<th>每百万页面费用（美元）</th>
</tr>
</thead>
<tbody>
<tr>
<td>a1.medium</td>
<td>100m 39.900s</td>
<td>41,294</td>
<td>24,612.725</td>
<td>1.03605</td>
</tr>
<tr>
<td>t2.small</td>
<td>78m 53.171s</td>
<td>41,200</td>
<td>31,336.286</td>
<td>0.73397</td>
</tr>
</tbody>
</table>
<p>为了方便比较，对于在 AWS 上跑的爬虫，我记录的指标和 Scaleway 上一致，但似乎没有达到预期的效果。这里我没有使用 <code>top</code>，而是使用了 AWS 提供的控制台来监控 CPU 的使用情况，从监控结果来看，我的爬虫程序并没有完全用到这两款服务器所提供的所有性能。</p>
<p>a1.medium 型号的机器尤为如此，在任务开始阶段，它的 CPU 使用率达到了峰值 45%，但随后一直在 20% 到 30% 之间。</p>
<p>让我有点感到意外的是，这个程序在 ARM 处理器上的运行速度相当慢，但却远未达到 Graviton CPU 能力的极限，而在 Intel Atom 处理器上则可以在某些时候达到 CPU 能力的极限。它们运行的代码是完全相同的，处理器的不同架构可能导致了对代码的不同处理方式。</p>
<p>个中原因无论是由于处理器本身的特性，还是二进制文件的编译，又或者是两者皆有，对我来说都是一个黑盒般的存在。我认为，既然在 AWS 机器上没有达到 CPU 处理能力的极限，那么只有在 Scaleway 机器上跑出来的性能数据是可以作为参考的。</p>
<p>t2.small 型号的机器性能让人费解。CPU 利用率大概 20%，最高才达到 35%，是因为手册中说的“20% 的基准性能，可以使用 CPU 积分突破这个基准”吗？但在控制台中可以看到 CPU 积分并没有被消耗。</p>
<p>为了确认这一点，我安装了 <a href="https://linux.die.net/man/1/stress" target="_blank" rel="noopener">stress</a> 这个软件，然后运行了一段时间，这个时候发现居然可以把 CPU 使用率提高到 100% 了。</p>
<p>显然，我需要调整一下它们的配置文件。我将 <code>CONCURRENT_REQUESTS</code> 参数设置为 5000，将 <code>REACTOR_THREADPOOL_MAXSIZE</code> 参数设置为 120，将爬虫任务的负载调得更大。</p>
<table>
<thead>
<tr>
<th>机器种类</th>
<th>耗时</th>
<th>爬取页面数</th>
<th>每小时爬取页面数</th>
<th>每万页面费用（美元）</th>
</tr>
</thead>
<tbody>
<tr>
<td>a1.medium</td>
<td>46m 13.619s</td>
<td>40,283</td>
<td>52,285.047</td>
<td>0.48771</td>
</tr>
<tr>
<td>t2.small</td>
<td>41m7.619s</td>
<td>36,241</td>
<td>52,871.857</td>
<td>0.43501</td>
</tr>
<tr>
<td>t2.small（无 CPU 积分）</td>
<td>73m 8.133s</td>
<td>34,298</td>
<td>28,137.8891</td>
<td>0.81740</td>
</tr>
</tbody>
</table>
<p>a1.medium 型号机器的 CPU 使用率在爬虫任务开始后 5 分钟飙升到了 100%，随后下降到 80% 并持续了 20 分钟，然后再次攀升到 96%，直到任务接近结束时再次下降。这大概就是我想要的效果了。</p>
<p>而 t2.small 型号机器在爬虫任务的前期就达到了 50%，并一直保持在这个水平直到任务接近结束。如果每个核心都有两个线程，那么 50% 的 CPU 使用率确实是单个线程可以达到的极限了。</p>
<p>现在我们看到它们的性能都差不多了。但至强处理器的线程持续跑满了 CPU，Graviton 处理器则只是有一段时间如此。可以认为 Graviton 略胜一筹。</p>
<p>然而，如果 CPU 积分耗尽了呢？这种情况下的对比可能更为公平。为了测试这种情况，我使用 <code>stress</code> 把所有的 CPU 积分用完，然后再次启动了爬虫任务。</p>
<p>在没有 CPU 积分的情况下，CPU 使用率在 27% 就到达极限不再上升了，同时又出现了丢失页面的现象。这么看来，它的性能比负载较低的时候更差。</p>
<h3 id="多线程爬虫"><a href="#多线程爬虫" class="headerlink" title="多线程爬虫"></a>多线程爬虫</h3><p>将爬虫任务分散到不同的进程中，可以有效利用机器所提供的多个核心。</p>
<p>一开始，我将爬虫任务分布在 10 个不同的进程中并同时启动，结果发现比每个核心仅使用 1 个进程的时候还要慢。</p>
<p>经过尝试，我得到了一个比较好的方案。把爬虫任务分布在 10 个进程中，但每个核心只启动 1 个进程，在每个进程接近结束的时候，再从剩余的进程中选出 1 个进程启动起来。</p>
<p>如果还需要优化，还可以让运行时间越长的爬虫进程在启动顺序中排得越靠前，我也在尝试实现这个方法。</p>
<p>想要预估某个域名的页面量，一定程度上可以参考这个域名主页的链接数量。我用另一个程序来对这个数量进行了统计，然后按照降序排序。经过这样的预处理之后，只会额外增加 1 分钟左右的时间。</p>
<p>结果，爬虫运行的总耗时超过了两个小时！毕竟把链接最多的域名都堆在同一个进程中也存在一定的弊端。</p>
<p>针对这个问题，也可以通过调整各个进程爬取的域名数量来进行优化，又或者在排序之后再作一定的修改。不过这种优化可能有点复杂了。</p>
<p>因此，我还是用回了最初的方法，它的效果还是相当不错的：</p>
<table>
<thead>
<tr>
<th>机器种类</th>
<th>耗时</th>
<th>爬取页面数</th>
<th>每小时爬取页面数</th>
<th>每万页面费用（欧元）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scaleway ARM64-2GB</td>
<td>62m 10.078s</td>
<td>36,158</td>
<td>34,897.0719</td>
<td>0.17193</td>
</tr>
<tr>
<td>Scaleway 1-S</td>
<td>60m 56.902s</td>
<td>36,725</td>
<td>36,153.5529</td>
<td>0.22128</td>
</tr>
</tbody>
</table>
<p>毕竟，使用多个核心能够大大加快爬虫的速度。</p>
<p>我认为，如果让一个经验丰富的程序员来优化的话，一定能够更好地利用所有的计算核心。但对于开箱即用的 Scrapy 来说，想要提高性能，使用更快的线程似乎比使用更多核心要简单得多。</p>
<p>从数量来看，Atom 处理器在更短的时间内爬取到了更多的页面。但如果从性价比角度来看，ThunderX 又是稍稍领先的。不过总的来说差距不大。</p>
<h3 id="爬取结果分析"><a href="#爬取结果分析" class="headerlink" title="爬取结果分析"></a>爬取结果分析</h3><p>在爬取了 38205 个页面之后，我们可以统计到在这些页面中 “ass” 出现了 24170435 次，而 “wood” 出现了 54368 次。</p>
<p><img src="https://blog.dxmtechsupport.com.au/wp-content/uploads/2019/02/Screenshot-from-2019-02-16-17-01-08.png" alt=""></p>
<p>“wood” 的出现次数不少，但和 “ass” 比起来简直微不足道。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>从上面的数据来看，对于性能而言，CPU 的架构并没有它们的问世时间重要，2018 年生产的 AWS Graviton 是单线程情况下性能最佳的。</p>
<p>你当然可以说按核心来比，Xeon 仍然赢了。但是，你不但需要计算美元的变化，甚至还要计算线程数。</p>
<p>另外在性能方面 2017 年生产的 Atom 轻松击败了 2014 年生产的 ThunderX，而 ThunderX 则在性价比方面占优。当然，如果你使用 AWS 的机器的话，还是使用 Graviton 吧。</p>
<p>总之，ARM 架构的硬件是可以用来运行爬虫程序的，而且在性能和费用方面也相当有竞争力。</p>
<p>而这种差异是否足以让你将整个技术架构迁移到 ARM 上？这就是另一回事了。当然，如果你已经是 AWS 用户，并且你的代码有很强的可移植性，那么不妨尝试一下 a1 型号的实例。</p>
<p>希望 ARM 设备在不久的将来能够在公有云上大放异彩。</p>
<h3 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h3><p>这是我第一次使用 Python 和 Scrapy 来做一个项目，所以我的代码写得可能不是很好，例如代码中使用全局变量就有点力不从心。</p>
<p>不过我仍然会在下面开源我的代码。</p>
<p>要运行这些代码，需要预先安装 Scrapy，并且需要 <a href="https://moz.com/top500" target="_blank" rel="noopener">Moz 上排名前 500 的网站</a>的 csv 文件。如果要运行 <code>butthead.py</code>，还需要安装 <a href="https://pypi.org/project/psutil/" target="_blank" rel="noopener">psutil</a> 这个库。</p>
<p><em>beavis.py</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line"></span><br><span class="line">ass = 0</span><br><span class="line">wood = 0</span><br><span class="line">totalpages = 0</span><br><span class="line"></span><br><span class="line">def getdomains():</span><br><span class="line"></span><br><span class="line">  moz500file = open(&apos;top500.domains.05.18.csv&apos;)</span><br><span class="line"></span><br><span class="line">  domains = []</span><br><span class="line">  moz500csv = moz500file.readlines()</span><br><span class="line"></span><br><span class="line">  del moz500csv[0]</span><br><span class="line"></span><br><span class="line">  for csvline in moz500csv:</span><br><span class="line">    leftquote = csvline.find(&apos;&quot;&apos;)    </span><br><span class="line">    rightquote = leftquote + csvline[leftquote + 1:].find(&apos;&quot;&apos;)</span><br><span class="line">    domains.append(csvline[leftquote + 1:rightquote])</span><br><span class="line"></span><br><span class="line">  return domains</span><br><span class="line"></span><br><span class="line">def getstartpages(domains):</span><br><span class="line">  </span><br><span class="line">  startpages = []</span><br><span class="line">  </span><br><span class="line">  for domain in domains:</span><br><span class="line">    startpages.append(&apos;http://&apos; + domain)</span><br><span class="line">  </span><br><span class="line">  return startpages</span><br><span class="line">  </span><br><span class="line">class AssWoodItem(scrapy.Item):</span><br><span class="line">  ass = scrapy.Field()</span><br><span class="line">  wood = scrapy.Field()</span><br><span class="line">  url = scrapy.Field()</span><br><span class="line">  </span><br><span class="line">class AssWoodPipeline(object):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    self.asswoodstats = []</span><br><span class="line"></span><br><span class="line">  def process_item(self, item, spider):</span><br><span class="line">    self.asswoodstats.append((item.get(&apos;url&apos;), item.get(&apos;ass&apos;), item.get(&apos;wood&apos;)))</span><br><span class="line">    </span><br><span class="line">  def close_spider(self, spider):</span><br><span class="line">    asstally, woodtally = 0, 0</span><br><span class="line">    </span><br><span class="line">    for asswoodcount in self.asswoodstats:</span><br><span class="line">      asstally += asswoodcount[1]</span><br><span class="line">      woodtally += asswoodcount[2]</span><br><span class="line">      </span><br><span class="line">    global ass, wood, totalpages</span><br><span class="line">    ass = asstally</span><br><span class="line">    wood = woodtally</span><br><span class="line">    totalpages = len(self.asswoodstats)</span><br><span class="line"></span><br><span class="line">class BeavisSpider(CrawlSpider):</span><br><span class="line">  name = &quot;Beavis&quot;</span><br><span class="line">  allowed_domains = getdomains()</span><br><span class="line">  start_urls = getstartpages(allowed_domains)</span><br><span class="line">  #start_urls = [ &apos;http://medium.com&apos; ]</span><br><span class="line">  custom_settings = &#123;</span><br><span class="line">    &apos;DEPTH_LIMIT&apos;: 3,</span><br><span class="line">    &apos;DOWNLOAD_DELAY&apos;: 3,</span><br><span class="line">    &apos;CONCURRENT_REQUESTS&apos;: 1500,</span><br><span class="line">    &apos;REACTOR_THREADPOOL_MAXSIZE&apos;: 60,</span><br><span class="line">    &apos;ITEM_PIPELINES&apos;: &#123; &apos;__main__.AssWoodPipeline&apos;: 10 &#125;,</span><br><span class="line">    &apos;LOG_LEVEL&apos;: &apos;INFO&apos;,</span><br><span class="line">    &apos;RETRY_ENABLED&apos;: False,</span><br><span class="line">    &apos;DOWNLOAD_TIMEOUT&apos;: 30,</span><br><span class="line">    &apos;COOKIES_ENABLED&apos;: False,</span><br><span class="line">    &apos;AJAXCRAWL_ENABLED&apos;: True</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  rules = ( Rule(LinkExtractor(), callback=&apos;parse_asswood&apos;), )</span><br><span class="line">  </span><br><span class="line">  def parse_asswood(self, response):</span><br><span class="line">    if isinstance(response, scrapy.http.TextResponse):</span><br><span class="line">      item = AssWoodItem()</span><br><span class="line">      item[&apos;ass&apos;] = response.text.casefold().count(&apos;ass&apos;)</span><br><span class="line">      item[&apos;wood&apos;] = response.text.casefold().count(&apos;wood&apos;)</span><br><span class="line">      item[&apos;url&apos;] = response.url</span><br><span class="line">      yield item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line"></span><br><span class="line">  process = CrawlerProcess(&#123;</span><br><span class="line">      &apos;USER_AGENT&apos;: &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&apos;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  process.crawl(BeavisSpider)</span><br><span class="line">  process.start()</span><br><span class="line"></span><br><span class="line">  print(&apos;Uhh, that was, like, &apos; + str(totalpages) + &apos; pages crawled.&apos;)</span><br><span class="line">  print(&apos;Uh huhuhuhuh. It said ass &apos; + str(ass) + &apos; times.&apos;)</span><br><span class="line">  print(&apos;Uh huhuhuhuh. It said wood &apos; + str(wood) + &apos; times.&apos;)</span><br></pre></td></tr></table></figure>
<p><em>butthead.py</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">import scrapy, time, psutil</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule, Spider</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line">from multiprocessing import Process, Queue, cpu_count</span><br><span class="line"></span><br><span class="line">ass = 0</span><br><span class="line">wood = 0</span><br><span class="line">totalpages = 0</span><br><span class="line">linkcounttuples =[]</span><br><span class="line"></span><br><span class="line">def getdomains():</span><br><span class="line"></span><br><span class="line">  moz500file = open(&apos;top500.domains.05.18.csv&apos;)</span><br><span class="line"></span><br><span class="line">  domains = []</span><br><span class="line">  moz500csv = moz500file.readlines()</span><br><span class="line"></span><br><span class="line">  del moz500csv[0]</span><br><span class="line"></span><br><span class="line">  for csvline in moz500csv:</span><br><span class="line">    leftquote = csvline.find(&apos;&quot;&apos;)    </span><br><span class="line">    rightquote = leftquote + csvline[leftquote + 1:].find(&apos;&quot;&apos;)</span><br><span class="line">    domains.append(csvline[leftquote + 1:rightquote])</span><br><span class="line"></span><br><span class="line">  return domains</span><br><span class="line"></span><br><span class="line">def getstartpages(domains):</span><br><span class="line">  </span><br><span class="line">  startpages = []</span><br><span class="line">  </span><br><span class="line">  for domain in domains:</span><br><span class="line">    startpages.append(&apos;http://&apos; + domain)</span><br><span class="line">  </span><br><span class="line">  return startpages</span><br><span class="line">  </span><br><span class="line">class AssWoodItem(scrapy.Item):</span><br><span class="line">  ass = scrapy.Field()</span><br><span class="line">  wood = scrapy.Field()</span><br><span class="line">  url = scrapy.Field()</span><br><span class="line">  </span><br><span class="line">class AssWoodPipeline(object):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    self.asswoodstats = []</span><br><span class="line"></span><br><span class="line">  def process_item(self, item, spider):</span><br><span class="line">    self.asswoodstats.append((item.get(&apos;url&apos;), item.get(&apos;ass&apos;), item.get(&apos;wood&apos;)))</span><br><span class="line">    </span><br><span class="line">  def close_spider(self, spider):</span><br><span class="line">    asstally, woodtally = 0, 0</span><br><span class="line">    </span><br><span class="line">    for asswoodcount in self.asswoodstats:</span><br><span class="line">      asstally += asswoodcount[1]</span><br><span class="line">      woodtally += asswoodcount[2]</span><br><span class="line">      </span><br><span class="line">    global ass, wood, totalpages</span><br><span class="line">    ass = asstally</span><br><span class="line">    wood = woodtally</span><br><span class="line">    totalpages = len(self.asswoodstats)</span><br><span class="line">          </span><br><span class="line"></span><br><span class="line">class ButtheadSpider(CrawlSpider):</span><br><span class="line">  name = &quot;Butthead&quot;</span><br><span class="line">  custom_settings = &#123;</span><br><span class="line">    &apos;DEPTH_LIMIT&apos;: 3,</span><br><span class="line">    &apos;DOWNLOAD_DELAY&apos;: 3,</span><br><span class="line">    &apos;CONCURRENT_REQUESTS&apos;: 250,</span><br><span class="line">    &apos;REACTOR_THREADPOOL_MAXSIZE&apos;: 30,</span><br><span class="line">    &apos;ITEM_PIPELINES&apos;: &#123; &apos;__main__.AssWoodPipeline&apos;: 10 &#125;,</span><br><span class="line">    &apos;LOG_LEVEL&apos;: &apos;INFO&apos;,</span><br><span class="line">    &apos;RETRY_ENABLED&apos;: False,</span><br><span class="line">    &apos;DOWNLOAD_TIMEOUT&apos;: 30,</span><br><span class="line">    &apos;COOKIES_ENABLED&apos;: False,</span><br><span class="line">    &apos;AJAXCRAWL_ENABLED&apos;: True</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  rules = ( Rule(LinkExtractor(), callback=&apos;parse_asswood&apos;), )</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  def parse_asswood(self, response):</span><br><span class="line">    if isinstance(response, scrapy.http.TextResponse):</span><br><span class="line">      item = AssWoodItem()</span><br><span class="line">      item[&apos;ass&apos;] = response.text.casefold().count(&apos;ass&apos;)</span><br><span class="line">      item[&apos;wood&apos;] = response.text.casefold().count(&apos;wood&apos;)</span><br><span class="line">      item[&apos;url&apos;] = response.url</span><br><span class="line">      yield item</span><br><span class="line"></span><br><span class="line">def startButthead(domainslist, urlslist, asswoodqueue):</span><br><span class="line">  crawlprocess = CrawlerProcess(&#123;</span><br><span class="line">      &apos;USER_AGENT&apos;: &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&apos;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  crawlprocess.crawl(ButtheadSpider, allowed_domains = domainslist, start_urls = urlslist)</span><br><span class="line">  crawlprocess.start()</span><br><span class="line">  asswoodqueue.put( (ass, wood, totalpages) )</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">  asswoodqueue = Queue()</span><br><span class="line">  domains=getdomains()</span><br><span class="line">  startpages=getstartpages(domains)</span><br><span class="line">  processlist =[]</span><br><span class="line">  cores = cpu_count()</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    domainsublist = domains[i * 50:(i + 1) * 50]</span><br><span class="line">    pagesublist = startpages[i * 50:(i + 1) * 50]</span><br><span class="line">    p = Process(target = startButthead, args = (domainsublist, pagesublist, asswoodqueue))</span><br><span class="line">    processlist.append(p)</span><br><span class="line">  </span><br><span class="line">  for i in range(cores):</span><br><span class="line">    processlist[i].start()</span><br><span class="line">    </span><br><span class="line">  time.sleep(180)</span><br><span class="line">  </span><br><span class="line">  i = cores</span><br><span class="line">  </span><br><span class="line">  while i != 10:</span><br><span class="line">    time.sleep(60)</span><br><span class="line">    if psutil.cpu_percent() &lt; 66.7:</span><br><span class="line">      processlist[i].start()</span><br><span class="line">      i += 1</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    processlist[i].join()</span><br><span class="line">  </span><br><span class="line">  for i in range(10):</span><br><span class="line">    asswoodtuple = asswoodqueue.get()</span><br><span class="line">    ass += asswoodtuple[0]</span><br><span class="line">    wood += asswoodtuple[1]</span><br><span class="line">    totalpages += asswoodtuple[2]</span><br><span class="line"></span><br><span class="line">  print(&apos;Uhh, that was, like, &apos; + str(totalpages) + &apos; pages crawled.&apos;)</span><br><span class="line">  print(&apos;Uh huhuhuhuh. It said ass &apos; + str(ass) + &apos; times.&apos;)</span><br><span class="line">  print(&apos;Uh huhuhuhuh. It said wood &apos; + str(wood) + &apos; times.&apos;)</span><br></pre></td></tr></table></figure>
<hr>
<p>via: <a href="https://blog.dxmtechsupport.com.au/speed-test-x86-vs-arm-for-web-crawling-in-python/" target="_blank" rel="noopener">https://blog.dxmtechsupport.com.au/speed-test-x86-vs-arm-for-web-crawling-in-python/</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
            <a href="/tags/LCTT-翻译/" rel="tag"># LCTT 翻译</a>
          
            <a href="/tags/x86/" rel="tag"># x86</a>
          
            <a href="/tags/ARM/" rel="tag"># ARM</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/17/All-about-Curly-Braces-in-Bash/" rel="next" title="浅析 Bash 中的 {花括号}">
                <i class="fa fa-chevron-left"></i> 浅析 Bash 中的 {花括号}
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/27/methods-and-pitfalls-on-maintaining-hexo-github-blog-from-multiple-devices/" rel="prev" title="在不同机器上维护 hexo github blog 的方法和坑">
                在不同机器上维护 hexo github blog 的方法和坑 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="HankChow">
            
              <p class="site-author-name" itemprop="name">HankChow</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">84</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">74</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/HankChow" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/HankZC" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i></a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是-ARM-架构处理器？"><span class="nav-number">1.</span> <span class="nav-text">什么是 ARM 架构处理器？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#服务器端的-ARM"><span class="nav-number">1.1.</span> <span class="nav-text">服务器端的 ARM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深入了解"><span class="nav-number">2.</span> <span class="nav-text">深入了解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scaleway"><span class="nav-number">2.1.</span> <span class="nav-text">Scaleway</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Amazon-Web-Services（AWS）"><span class="nav-number">2.2.</span> <span class="nav-text">Amazon Web Services（AWS）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#瘪四与大头蛋爬取-Moz-排行榜前-500-的网站"><span class="nav-number">3.</span> <span class="nav-text">瘪四与大头蛋爬取 Moz 排行榜前 500 的网站</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在-ARM-服务器上安装-Scrapy"><span class="nav-number">4.</span> <span class="nav-text">在 ARM 服务器上安装 Scrapy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#单线程爬虫"><span class="nav-number">5.</span> <span class="nav-text">单线程爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多线程爬虫"><span class="nav-number">6.</span> <span class="nav-text">多线程爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取结果分析"><span class="nav-number">7.</span> <span class="nav-text">爬取结果分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-number">8.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#源代码"><span class="nav-number">9.</span> <span class="nav-text">源代码</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-home"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HankChow</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">111.3k</span>
  
</div>









        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
